package com.tw ter.graph.batch.job.t epcred

 mport com.tw ter.scald ng._

/**
 *   ghted page rank for t  g ven graph, start from t  g ven pagerank,
 * perform one  erat on, test for convergence,  f not yet, clone  self
 * and start t  next page rank job w h updated pagerank as  nput;
 *  f converged, start ExtractT epcred job  nstead
 *
 * Opt ons:
 * --pwd: work ng d rectory, w ll read/generate t  follow ng f les t re
 *        numnodes: total number of nodes
 *        nodes: nodes f le <'src_ d, 'dst_ ds, '  ghts, 'mass_pr or>
 *        pagerank: t  page rank f le eg pagerank_0, pagerank_1 etc
 *        totald ff: t  current max pagerank delta
 * Opt onal argu nts:
 * --  ghted: do   ghted pagerank, default false
 * --cur erat on: what  s t  current  erat on, default 0
 * --max erat ons: how many  erat ons to run.  Default  s 20
 * --jumpprob: probab l y of a random jump, default  s 0.1
 * --threshold: total d fference before f n sh ng early, default 0.001
 *
 * plus t  follow ng opt ons for ExtractT epcred:
 * --user_mass: user mass tsv f le, generated by twadoop user_mass job
 * --output_pagerank: w re to put pagerank f le
 * --output_t epcred: w re to put t epcred f le
 * Opt onal:
 * --post_adjust: w t r to do post adjust, default true
 *
 */
class   ghtedPageRank(args: Args) extends Job(args) {
  val ROW_TYPE_1 = 1
  val ROW_TYPE_2 = 2

  val PWD = args("pwd")
  val ALPHA = args.getOrElse("jumpprob", "0.1").toDouble
  val WE GHTED = args.getOrElse("  ghted", "false").toBoolean
  val THRESHOLD = args.getOrElse("threshold", "0.001").toDouble
  val MAX TERAT ONS = args.getOrElse("max erat ons", "20").to nt
  val CUR TERAT ON = args.getOrElse("cur erat on", "0").to nt

  // 's ze
  val numNodes = getNumNodes(PWD + "/numnodes")

  // 'src_ d, 'dst_ ds, '  ghts, 'mass_pr or
  val nodes = getNodes(PWD + "/nodes")

  // 'src_ d_ nput, 'mass_ nput
  val  nputPagerank = get nputPagerank(PWD + "/pagerank_" + CUR TERAT ON)

  // one  erat on of pagerank
  val outputPagerank = doPageRank(nodes,  nputPagerank)
  val outputF leNa  = PWD + "/pagerank_" + (CUR TERAT ON + 1)
  outputPagerank
    .project('src_ d, 'mass_n)
    .wr e(Tsv(outputF leNa ))

  // detect convergence
  val totalD ff = outputPagerank
    .mapTo(('mass_ nput, 'mass_n) -> 'mass_d ff) { args: (Double, Double) =>
      scala.math.abs(args._1 - args._2)
    }
    .groupAll { _.sum[Double]('mass_d ff) }
    .wr e(Tsv(PWD + "/totald ff"))

  /**
   * test convergence,  f not yet, k ck off t  next  erat on
   */
  overr de def next = {
    // t  max d ff generated above
    val totalD ff = Tsv(PWD + "/totald ff").readAtSubm ter[Double]. ad

     f (CUR TERAT ON < MAX TERAT ONS - 1 && totalD ff > THRESHOLD) {
      val newArgs = args + ("cur erat on", So ((CUR TERAT ON + 1).toStr ng))
      So (clone(newArgs))
    } else {
      val newArgs = args + (" nput_pagerank", So (outputF leNa ))
      So (new ExtractT epcred(newArgs))
    }
  }

  def get nputPagerank(f leNa : Str ng) = {
    Tsv(f leNa ).read
      .mapTo((0, 1) -> ('src_ d_ nput, 'mass_ nput)) {  nput: (Long, Double) =>
         nput
      }
  }

  /**
   * read t  pregenerated nodes f le <'src_ d, 'dst_ ds, '  ghts, 'mass_pr or>
   */
  def getNodes(f leNa : Str ng) = {
    mode match {
      case Hdfs(_, conf) => {
        SequenceF le(f leNa ).read
          .mapTo((0, 1, 2, 3) -> ('src_ d, 'dst_ ds, '  ghts, 'mass_pr or)) {
             nput: (Long, Array[Long], Array[Float], Double) =>
               nput
          }
      }
      case _ => {
        Tsv(f leNa ).read
          .mapTo((0, 1, 2, 3) -> ('src_ d, 'dst_ ds, '  ghts, 'mass_pr or)) {
             nput: (Long, Str ng, Str ng, Double) =>
              {
                (
                   nput._1,
                  // convert str ng to  nt array
                   f ( nput._2 != null &&  nput._2.length > 0) {
                     nput._2.spl (",").map { _.toLong }
                  } else {
                    Array[Long]()
                  },
                  // convert str ng to float array
                   f ( nput._3 != null &&  nput._3.length > 0) {
                     nput._3.spl (",").map { _.toFloat }
                  } else {
                    Array[Float]()
                  },
                   nput._4
                )
              }
          }
      }
    }
  }

  /**
   * t  total number of nodes, s ngle l ne f le
   */
  def getNumNodes(f leNa : Str ng) = {
    Tsv(f leNa ).read
      .mapTo(0 -> 's ze) {  nput: Long =>
         nput
      }
  }

  /**
   * one  erat on of pagerank
   *  nputPagerank: <'src_ d_ nput, 'mass_ nput>
   * return <'src_ d, 'mass_n, 'mass_ nput>
   *
   *  re  s a h ghlevel v ew of t  un  ghted algor hm:
   * let
   * N: number of nodes
   *  nputPagerank(N_ ): prob of walk ng to node  ,
   * d(N_j): N_j's out degree
   * t n
   * pagerankNext(N_ ) = (\sum_{j po nts to  }  nputPagerank(N_j) / d_j)
   * deadPagerank = (1 - \sum_{ } pagerankNext(N_ )) / N
   * randomPagerank(N_ ) = userMass(N_ ) * ALPHA + deadPagerank * (1-ALPHA)
   * pagerankOutput(N_ ) = randomPagerank(N_ ) + pagerankNext(N_ ) * (1-ALPHA)
   *
   * For   ghted algor hm:
   * let
   * w(N_j, N_ ):   ght from N_j to N_ 
   * tw(N_j): N_j's total out   ghts
   * t n
   * pagerankNext(N_ ) = (\sum_{j po nts to  }  nputPagerank(N_j) * w(N_j, N_ ) / tw(N_j))
   *
   */
  def doPageRank(nodeRows: R chP pe,  nputPagerank: R chP pe): R chP pe = {
    // 'src_ d, 'dst_ ds, '  ghts, 'mass_pr or, 'mass_ nput
    val nodeJo ned = nodeRows
      .jo nW hSmaller('src_ d -> 'src_ d_ nput,  nputPagerank)
      .d scard('src_ d_ nput)

    // 'src_ d, 'mass_n
    val pagerankNext = nodeJo ned
      .flatMapTo(('dst_ ds, '  ghts, 'mass_ nput) -> ('src_ d, 'mass_n)) {
        args: (Array[Long], Array[Float], Double) =>
          {
             f (args._1.length > 0) {
               f (WE GHTED) {
                //   ghted d str but on
                val total: Double = args._2.sum
                (args._1 z p args._2).map {  d  ght: (Long, Float) =>
                  ( d  ght._1, args._3 *  d  ght._2 / total)
                }
              } else {
                // equal d str but on
                val d st: Double = args._3 / args._1.length
                args._1.map {  d: Long =>
                  ( d, d st)
                }
              }
            } else {
              // re  s a node that po nts to no ot r nodes (dangl ng)
              N l
            }
          }
      }
      .groupBy('src_ d) {
        _.sum[Double]('mass_n)
      }

    // 'sum_mass
    val sumPagerankNext = pagerankNext.groupAll { _.sum[Double]('mass_n -> 'sum_mass) }

    // 'deadMass
    // s ngle row jobs
    // t  dead page rank equally d str buted to every node
    val deadPagerank = sumPagerankNext
      .crossW hT ny(numNodes)
      .map(('sum_mass, 's ze) -> 'deadMass) {  nput: (Double, Long) =>
        (1.0 -  nput._1) /  nput._2
      }
      .d scard('s ze, 'sum_mass)

    // 'src_ d_r, 'mass_n_r
    // random jump probab l y plus dead page rank
    val randomPagerank = nodeJo ned
      .crossW hT ny(deadPagerank)
      .mapTo(('src_ d, 'mass_pr or, 'deadMass, 'mass_ nput) -> ('src_ d, 'mass_n, 'mass_ nput)) {
        ranks: (Long, Double, Double, Double) =>
          (ranks._1, ranks._2 * ALPHA + ranks._3 * (1 - ALPHA), ranks._4)
      }

    // 'src_ d, 'mass_n
    // scale next page rank to 1-ALPHA
    val pagerankNextScaled = pagerankNext
      .map('mass_n -> ('mass_n, 'mass_ nput)) { m: Double =>
        ((1 - ALPHA) * m, 0.0)
      }

    // 'src_ d, 'mass_n, 'mass_ nput
    // random probab l y + next probab l y
    (randomPagerank ++ pagerankNextScaled)
      .groupBy('src_ d) {
        _.sum[Double]('mass_ nput) // keep t   nput pagerank
          .sum[Double]('mass_n) // take t  sum
      }
  }
}
