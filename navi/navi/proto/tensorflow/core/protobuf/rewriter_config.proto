syntax = "proto3";

package tensorflow;

 mport "tensorflow/core/fra work/attr_value.proto";
 mport "tensorflow/core/protobuf/ver f er_conf g.proto";

opt on cc_enable_arenas = true;
opt on java_outer_classna  = "Rewr erConf gProtos";
opt on java_mult ple_f les = true;
opt on java_package = "org.tensorflow.fra work";
opt on go_package = "g hub.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto";

 ssage AutoParallelOpt ons {
  bool enable = 1;
   nt32 num_repl cas = 2;
}

 ssage ScopedAllocatorOpt ons {
  //  f present, only perform opt m zat on for t se ops.
  repeated str ng enable_op = 1;
}

 ssage Rewr erConf g {
  // Graph rewr  ng  s exper  ntal and subject to change, not covered by any
  // AP  stab l y guarantees.

  // Conf gurat on opt ons for t   ta-opt m zer. Unless ot rw se noted, t se
  // conf gurat on opt ons do not apply to expl c ly tr ggered opt m zat on
  // passes  n t  opt m zers f eld.

  enum Toggle {
    DEFAULT = 0;
    ON = 1;
    OFF = 2;
    // Enable so  aggress ve opt m zat ons that use assumpt ons that TF graphs
    // may break. For example, assu  t  shape of a placeholder matc s  s
    // actual feed.
    AGGRESS VE = 3;
  }

  // Enum for la t convers on bet en NCHW and NHWC on CPU. Default  s OFF.
  enum CpuLa t {
    NO_CONVERS ON_ON_CPU = 0;
    NCHW_TO_NHWC = 1;
    NHWC_TO_NCHW = 2;
  }

  // Enum controll ng t  number of t  s to run opt m zers. T  default  s to
  // run t m tw ce.
  enum Num erat onsType {
    DEFAULT_NUM_ TERS = 0;
    ONE = 1;
    TWO = 2;
  }

  // CPU Convers on sett ngs bet en NHCW and NCHW.
  CpuLa t cpu_la t_convers on = 50;

  // Opt m ze tensor la ts (default  s ON)
  // e.g. T  w ll try to use NCHW la t on GPU wh ch  s faster.
  Toggle la t_opt m zer = 1;
  // Fold constants (default  s ON)
  // Stat cally  nfer t  value of tensors w n poss ble, and mater al ze t 
  // result us ng constants.
  Toggle constant_fold ng = 3;
  // Shape opt m zat ons (default  s ON)
  // S mpl fy computat ons made on shapes.
  Toggle shape_opt m zat on = 13;
  // Remapp ng (default  s ON)
  // Remap subgraphs onto more eff c ent  mple ntat ons.
  Toggle remapp ng = 14;
  // Common subgraph el m nat on (default  s ON)
  // e.g. S mpl fy ar h t c ops;  rge ops w h sa  value (l ke constants).
  Toggle common_subgraph_el m nat on = 24;
  // Ar h t c opt m zat ons (default  s ON)
  // e.g. S mpl fy ar h t c ops;  rge ops w h sa  value (l ke constants).
  Toggle ar h t c_opt m zat on = 7;
  // Control dependency opt m zat ons (default  s ON).
  // Remove redundant control dependenc es, wh ch may enable ot r opt m zat on.
  Toggle dependency_opt m zat on = 8;
  // Loop opt m zat ons (default  s ON).
  Toggle loop_opt m zat on = 9;
  // Funct on opt m zat ons (default  s ON).
  Toggle funct on_opt m zat on = 10;
  // Str ps debug-related nodes from t  graph (off by default).
  Toggle debug_str pper = 11;
  //  f true, don't remove unnecessary ops from t  graph
  bool d sable_model_prun ng = 2;
  // Try to allocate so   ndependent Op outputs cont guously  n order to
  //  rge or el m nate downstream Ops (off by default).
  Toggle scoped_allocator_opt m zat on = 15;
  // Force small ops onto t  CPU (default  s OFF).
  Toggle p n_to_host_opt m zat on = 18;
  // Enable t  swap of kernel  mple ntat ons based on t  dev ce place nt
  // (default  s ON).
  Toggle  mple ntat on_selector = 22;
  // Opt m ze data types for CUDA (default  s OFF).
  // T  w ll try to use float16 on GPU wh ch  s faster.
  // Note that t  can change t  nu r cal stab l y of t  graph and may
  // requ re t  use of loss scal ng to ma nta n model convergence.
  Toggle auto_m xed_prec s on = 23;
  // Opt m ze data types for MKL (default  s OFF).
  // T  w ll try to use bfloat16 on CPUs, wh ch  s faster.
  // Note that t  can change t  nu r cal stab l y of t  graph.
  Toggle auto_m xed_prec s on_mkl = 25;
  // Emulate a model us ng data type float16 on CPU (default  s OFF).
  // T  w ll try to emulate t  float16  nputs and outputs of an operator
  // on CPU to have better correlat on w h float16 on GPU; ho ver t 
  // computat on  n t  operator  s based on float32.
  // Note that t  can change t  nu r cal stab l y of t  graph.
  Toggle auto_m xed_prec s on_cpu = 29;
  // D sable t  ent re  ta opt m zer (off by default).
  bool d sable_ ta_opt m zer = 19;
  // Opt m zers reg stered by plug n (default  s ON)
  Toggle use_plug n_opt m zers = 28;

  // Controls how many t  s   run t  opt m zers  n  ta opt m zer (default
  //  s once).
  Num erat onsType  ta_opt m zer_ erat ons = 12;

  // T  m n mum number of nodes  n a graph to opt m zer. For smaller graphs,
  // opt m zat on  s sk pped.
  // 0  ans t  system p cks an appropr ate number.
  // < 0  ans do not sk p opt m zat on.
   nt32 m n_graph_nodes = 17;

  // D sable opt m zat ons that assu  compressed tensors. Note that t  flag
  //  s exper  ntal and may be removed  n t  future.
  bool exper  ntal_d sable_compressed_tensor_opt m zat on = 26;

  // D sable fold ng quant zat on emulat on ops such as FakeQuantW hM nMax* and
  // Quant zeAndDequant ze*. So  comp lers (e.g. t  TF-to-tfl e converter)
  // have to extract quant zat on conf gs (e.g. m n/max range, number of b s,
  // and per-channel) from t  quant zat on emulat on ops. Note that t  flag
  //  s exper  ntal and may be removed  n t  future. See b/174138564 for more
  // deta ls.
  bool exper  ntal_d sable_fold ng_quant zat on_emulat on = 27;

  enum  mOptType {
    // T  default sett ng (SCHEDUL NG and SWAPP NG HEUR ST CS only)
    DEFAULT_MEM_OPT = 0;
    // D sabled  n t   ta-opt m zer.
    NO_MEM_OPT = 1;
    // Dr ven by manual op-level annotat ons.
    MANUAL = 2;

    // Dr ven by  ur st cs. T  behav or of t se  ur st cs  s subject to
    // change. Currently  ncludes an exper  ntal recomputat on and swapp ng
    //  ur st cs. Manual annotat ons are respected, but add  onal nodes are
    // selected automat cally.

    // Swapp ng  ur st c w ll move a tensor from t  GPU to t  CPU and move
    //   back w n needed to reduce peak  mory usage.
    SWAPP NG_HEUR ST CS = 4;
    // Recomputat on  ur st cs w ll recompute ops (such as Relu act vat on)
    // dur ng backprop  nstead of stor ng t m, reduc ng peak  mory usage.
    RECOMPUTAT ON_HEUR ST CS = 5;
    // Sc dul ng w ll spl  b g ops such as AddN and try to enforce a sc dule
    // of t  new computat ons that decreases peak  mory usage.
    SCHEDUL NG_HEUR ST CS = 6;
    // Use any comb nat on of swapp ng and recomputat on  ur st cs.
    HEUR ST CS = 3;
  }
  // Conf gures  mory opt m zat on passes through t   ta-opt m zer. Has no
  // effect on manually requested  mory opt m zat on passes  n t  opt m zers
  // f eld.
   mOptType  mory_opt m zat on = 4;
  // A node na  scope for node na s wh ch are val d outputs of recomputat ons.
  //  nputs to nodes that match t  scope may be recomputed (subject e  r to
  // manual annotat on of those  nput nodes or to manual annotat on and
  //  ur st cs depend ng on  mory_opt m zat on), but t  nodes t mselves w ll
  // not be recomputed. T  matc s any sub-scopes as  ll,  an ng t  scope
  // can appear not just as a top-level scope. For example,  f t  value  s
  // "grad ents/", t  default,   w ll match node na  "grad ents/foo",
  // "foo/grad ents/bar", but not "foo_grad ents/"
  str ng  mory_opt m zer_target_node_na _scope = 6;
  // Max mum number of m ll seconds to spend opt m z ng a s ngle graph before
  // t m ng out.  f less than or equal to 0 (default value) t  opt m zer w ll
  // never t   out.
   nt64  ta_opt m zer_t  out_ms = 20;

  // Conf gures AutoParallel opt m zat on passes e  r through t 
  //  ta-opt m zer or w n manually spec f ed through t  opt m zers f eld.
  AutoParallelOpt ons auto_parallel = 5;

  //  f true, any opt m zat on pass fa l ng w ll cause t   taOpt m zer to
  // stop w h an error. By default - or w n set to false, fa l ng passes are
  // sk pped s lently.
  bool fa l_on_opt m zer_errors = 21;

  ScopedAllocatorOpt ons scoped_allocator_opts = 16;

  //  f non-empty, w ll use t  as an alternat ve way to spec fy a l st of
  // opt m zat ons to turn on and t  order of t  opt m zat ons (replac ng t 
  //  ta-opt m zer).
  //
  // Of t  Rewr erConf g opt ons, only t  AutoParallel conf gurat on opt ons
  // (t  auto_parallel f eld) apply to manually requested opt m zat on passes
  // ("autoparallel").  mory opt m zat on passes (" mory")  nvoked  re are
  // not conf gurable ( n contrast to  mory opt m zat on passes through t 
  //  ta-opt m zer) and act only on manual op annotat ons.
  //
  // Custom opt m zers (see custom_opt m zers) that are not part of t 
  // sc dule w ll be run after -  n t  order that t y  re spec f ed.
  repeated str ng opt m zers = 100;

  //  ssage to descr be custom graph opt m zer and  s para ters
   ssage CustomGraphOpt m zer {
    str ng na  = 1;
    map<str ng, AttrValue> para ter_map = 2;
  }

  // l st of CustomGraphOpt m zers to apply.
  repeated CustomGraphOpt m zer custom_opt m zers = 200;

  // Ver f erConf g spec fy ng t  ver f ers to be run after every opt m zer.
  Ver f erConf g  nter_opt m zer_ver f er_conf g = 300;

  // Ver f erConf g spec fy ng t  ver f ers to be run at t  end, after all
  // opt m zers have run.
  Ver f erConf g post_opt m zat on_ver f er_conf g = 301;
}
