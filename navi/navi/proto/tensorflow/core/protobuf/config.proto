syntax = "proto3";

package tensorflow;

 mport "tensorflow/core/fra work/cost_graph.proto";
 mport "tensorflow/core/fra work/graph.proto";
 mport "tensorflow/core/fra work/step_stats.proto";
 mport "tensorflow/core/protobuf/cluster.proto";
 mport "tensorflow/core/protobuf/coord nat on_conf g.proto";
 mport "tensorflow/core/protobuf/debug.proto";
 mport "tensorflow/core/protobuf/rewr er_conf g.proto";

opt on cc_enable_arenas = true;
opt on java_outer_classna  = "Conf gProtos";
opt on java_mult ple_f les = true;
opt on java_package = "org.tensorflow.fra work";
opt on go_package = "g hub.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto";

 ssage GPUOpt ons {
  // Fract on of t  ava lable GPU  mory to allocate for each process.
  // 1  ans to allocate all of t  GPU  mory, 0.5  ans t  process
  // allocates up to ~50% of t  ava lable GPU  mory.
  //
  // GPU  mory  s pre-allocated unless t  allow_growth opt on  s enabled.
  //
  //  f greater than 1.0, uses CUDA un f ed  mory to potent ally oversubscr be
  // t  amount of  mory ava lable on t  GPU dev ce by us ng host  mory as a
  // swap space. Access ng  mory not ava lable on t  dev ce w ll be
  // s gn f cantly slo r as that would requ re  mory transfer bet en t  host
  // and t  dev ce. Opt ons to reduce t   mory requ re nt should be
  // cons dered before enabl ng t  opt on as t  may co  w h a negat ve
  // performance  mpact. Oversubscr pt on us ng t  un f ed  mory requ res
  // Pascal class or ne r GPUs and    s currently only supported on t  L nux
  // operat ng system. See
  // https://docs.nv d a.com/cuda/cuda-c-programm ng-gu de/ ndex.html#um-requ re nts
  // for t  deta led requ re nts.
  double per_process_gpu_ mory_fract on = 1;

  //  f true, t  allocator does not pre-allocate t  ent re spec f ed
  // GPU  mory reg on,  nstead start ng small and grow ng as needed.
  bool allow_growth = 4;

  // T  type of GPU allocat on strategy to use.
  //
  // Allo d values:
  // "": T  empty str ng (default) uses a system-chosen default
  //     wh ch may change over t  .
  //
  // "BFC": A "Best-f  w h coalesc ng" algor hm, s mpl f ed from a
  //        vers on of dlmalloc.
  str ng allocator_type = 2;

  // Delay delet on of up to t  many bytes to reduce t  number of
  //  nteract ons w h gpu dr ver code.   f 0, t  system chooses
  // a reasonable default (several MBs).
   nt64 deferred_delet on_bytes = 3;

  // A comma-separated l st of GPU  ds that determ nes t  'v s ble'
  // to 'v rtual' mapp ng of GPU dev ces.  For example,  f TensorFlow
  // can see 8 GPU dev ces  n t  process, and one wanted to map
  // v s ble GPU dev ces 5 and 3 as "/dev ce:GPU:0", and "/dev ce:GPU:1",
  // t n one would spec fy t  f eld as "5,3".  T  f eld  s s m lar  n
  // sp r  to t  CUDA_V S BLE_DEV CES env ron nt var able, except
  //   appl es to t  v s ble GPU dev ces  n t  process.
  //
  // NOTE:
  // 1. T  GPU dr ver prov des t  process w h t  v s ble GPUs
  //     n an order wh ch  s not guaranteed to have any correlat on to
  //    t  *phys cal* GPU  d  n t  mach ne.  T  f eld  s used for
  //    remapp ng "v s ble" to "v rtual", wh ch  ans t  operates only
  //    after t  process starts.  Users are requ red to use vendor
  //    spec f c  chan sms (e.g., CUDA_V S BLE_DEV CES) to control t 
  //    phys cal to v s ble dev ce mapp ng pr or to  nvok ng TensorFlow.
  // 2.  n t  code, t   ds  n t  l st are also called "platform GPU  d"s,
  //    and t  'v rtual'  ds of GPU dev ces ( .e. t   ds  n t  dev ce
  //    na  "/dev ce:GPU:< d>") are also called "TF GPU  d"s. Please
  //    refer to th rd_party/tensorflow/core/common_runt  /gpu/gpu_ d.h
  //    for more  nformat on.
  str ng v s ble_dev ce_l st = 5;

  //  n t  event poll ng loop sleep t  many m croseconds bet en
  // PollEvents calls, w n t  queue  s not empty.   f value  s not
  // set or set to 0, gets set to a non-zero default.
   nt32 poll ng_act ve_delay_usecs = 6;

  // T  f eld  s deprecated and  gnored.
   nt32 poll ng_ nact ve_delay_msecs = 7;

  // Force all tensors to be gpu_compat ble. On a GPU-enabled TensorFlow,
  // enabl ng t  opt on forces all CPU tensors to be allocated w h Cuda
  // p nned  mory. Normally, TensorFlow w ll  nfer wh ch tensors should be
  // allocated as t  p nned  mory. But  n case w re t   nference  s
  //  ncomplete, t  opt on can s gn f cantly speed up t  cross-dev ce  mory
  // copy performance as long as   f s t   mory.
  // Note that t  opt on  s not so th ng that should be
  // enabled by default for unknown or very large models, s nce all Cuda p nned
  //  mory  s unpageable, hav ng too much p nned  mory m ght negat vely  mpact
  // t  overall host system performance.
  bool force_gpu_compat ble = 8;

   ssage Exper  ntal {
    // Conf gurat on for break ng down a v s ble GPU  nto mult ple "v rtual"
    // dev ces.
     ssage V rtualDev ces {
      // Per "v rtual" dev ce  mory l m ,  n MB. T  number of ele nts  n
      // t  l st  s t  number of v rtual dev ces to create on t 
      // correspond ng v s ble GPU (see "v rtual_dev ces" below).
      //  f empty,   w ll create s ngle v rtual dev ce tak ng all ava lable
      //  mory from t  dev ce.
      //
      // For t  concept of "v s ble" and "v rtual" GPU, see t  com nts for
      // "v s ble_dev ce_l st" above for more  nformat on.
      repeated float  mory_l m _mb = 1;

      // Pr or y values to use w h t  v rtual dev ces. Use t  cuda funct on
      // cudaDev ceGetStreamPr or yRange to query for val d range of values for
      // pr or y.
      //
      // On a P4000 GPU w h cuda 10.1, t  pr or y range reported was 0 for
      // least pr or y and -1 for greatest pr or y.
      //
      //  f t  f eld  s not spec f ed, t n t  v rtual dev ces w ll be
      // created w h t  default.  f t  f eld has values set, t n t  s ze
      // of t  must match w h t  above  mory_l m _mb.
      repeated  nt32 pr or y = 2;
    }

    // T  mult  v rtual dev ce sett ngs.  f empty (not set),   w ll create
    // s ngle v rtual dev ce on each v s ble GPU, accord ng to t  sett ngs
    //  n "v s ble_dev ce_l st" above. Ot rw se, t  number of ele nts  n t 
    // l st must be t  sa  as t  number of v s ble GPUs (after
    // "v s ble_dev ce_l st" f lter ng  f    s set), and t  str ng represented
    // dev ce na s (e.g. /dev ce:GPU:< d>) w ll refer to t  v rtual
    // dev ces and have t  < d> f eld ass gned sequent ally start ng from 0,
    // accord ng to t  order t y appear  n t  l st and t  " mory_l m "
    // l st  ns de each ele nt. For example,
    //   v s ble_dev ce_l st = "1,0"
    //   v rtual_dev ces {  mory_l m : 1GB  mory_l m : 2GB }
    //   v rtual_dev ces {}
    // w ll create three v rtual dev ces as:
    //   /dev ce:GPU:0 -> v s ble GPU 1 w h 1GB  mory
    //   /dev ce:GPU:1 -> v s ble GPU 1 w h 2GB  mory
    //   /dev ce:GPU:2 -> v s ble GPU 0 w h all ava lable  mory
    //
    // NOTE:
    // 1.  's  nval d to set both t  and "per_process_gpu_ mory_fract on"
    //    at t  sa  t  .
    // 2. Currently t  sett ng  s per-process, not per-sess on. Us ng
    //    d fferent sett ngs  n d fferent sess ons w h n sa  process w ll
    //    result  n undef ned behav or.
    repeated V rtualDev ces v rtual_dev ces = 1;

    //  f true, uses CUDA un f ed  mory for  mory allocat ons.  f
    // per_process_gpu_ mory_fract on opt on  s greater than 1.0, t n un f ed
    //  mory  s used regardless of t  value for t  f eld. See com nts for
    // per_process_gpu_ mory_fract on f eld for more deta ls and requ re nts
    // of t  un f ed  mory. T  opt on  s useful to oversubscr be  mory  f
    // mult ple processes are shar ng a s ngle GPU wh le  nd v dually us ng less
    // than 1.0 per process  mory fract on.
    bool use_un f ed_ mory = 2;

    //  f > 1, t  number of dev ce-to-dev ce copy streams to create
    // for each GPUDev ce.  Default value  s 0, wh ch  s automat cally
    // converted to 1.
     nt32 num_dev_to_dev_copy_streams = 3;

    //  f non-empty, def nes a good GPU r ng order on a s ngle worker based on
    // dev ce  nterconnect.  T  assu s that all workers have t  sa  GPU
    // topology.  Spec fy as a comma-separated str ng, e.g. "3,2,1,0,7,6,5,4".
    // T  r ng order  s used by t  R ngReducer  mple ntat on of
    // Collect veReduce, and serves as an overr de to automat c r ng order
    // generat on  n OrderTaskDev ceMap() dur ng Collect veParam resolut on.
    str ng collect ve_r ng_order = 4;

    //  f true t n extra work  s done by GPUDev ce and GPUBFCAllocator to
    // keep track of w n GPU  mory  s freed and w n kernels actually
    // complete so that   can know w n a nom nally free  mory chunk
    //  s really not subject to pend ng use.
    bool t  stamped_allocator = 5;

    // reserved  d: 6

    // Para ters for GPUKernelTracker.  By default no kernel track ng  s done.
    // Note that t  stamped_allocator  s only effect ve  f so  track ng  s
    // spec f ed.
    //
    //  f kernel_tracker_max_ nterval = n > 0, t n a track ng event
    //  s  nserted after every n kernels w hout an event.
     nt32 kernel_tracker_max_ nterval = 7;
    //  f kernel_tracker_max_bytes = n > 0, t n a track ng event  s
    //  nserted after every ser es of kernels allocat ng a sum of
    //  mory >= n.   f one kernel allocates b * n bytes, t n one
    // event w ll be  nserted after  , but   w ll count as b aga nst
    // t  pend ng l m .
     nt32 kernel_tracker_max_bytes = 8;
    //  f kernel_tracker_max_pend ng > 0 t n no more than t  many
    // track ng events can be outstand ng at a t  .  An attempt to
    // launch an add  onal kernel w ll stall unt l an event
    // completes.
     nt32 kernel_tracker_max_pend ng = 9;

    // BFC Allocator can return an allocated chunk of  mory upto 2x t 
    // requested s ze. For v rtual dev ces w h t ght  mory constra nts, and
    // proport onately large allocat on requests, t  can lead to a s gn f cant
    // reduct on  n ava lable  mory. T  threshold below controls w n a chunk
    // should be spl   f t  chunk s ze exceeds requested  mory s ze.    s
    // expressed as a fract on of total ava lable  mory for t  tf dev ce. For
    // example sett ng   to 0.05 would  mply a chunk needs to be spl   f  s
    // s ze exceeds t  requested  mory by 5% of t  total v rtual dev ce/gpu
    //  mory s ze.
    double  nternal_frag ntat on_fract on = 10;

    // W n true, use CUDA cudaMallocAsync AP   nstead of TF gpu allocator.
    bool use_cuda_malloc_async = 11;

    // By default, BFCAllocator may sleep w n   runs out of  mory,  n t 
    // hopes that anot r thread w ll free up  mory  n t   ant  .  Sett ng
    // t  to true d sables t  sleep;  nstead  'll OOM  m d ately.
    bool d sallow_retry_on_allocat on_fa lure = 12;
  }

  // Everyth ng  ns de exper  ntal  s subject to change and  s not subject
  // to AP  stab l y guarantees  n
  // https://www.tensorflow.org/gu de/vers on_compat.
  Exper  ntal exper  ntal = 9;
}

// Opt ons passed to t  graph opt m zer
 ssage Opt m zerOpt ons {
  //  f true, opt m ze t  graph us ng common subexpress on el m nat on.
  // Note: t  opt m zat on Level L1 w ll overr de t  sett ng to true. So  n
  // order to d sable common subexpress on el m nat on t  opt_level has to be
  // set to L0.
  bool do_common_subexpress on_el m nat on = 1;

  //  f true, perform constant fold ng opt m zat on on t  graph.
  // Note: t  opt m zat on Level L1 w ll overr de t  sett ng to true. So  n
  // order to d sable constant fold ng t  opt_level has to be set to L0.
  bool do_constant_fold ng = 2;

  // Constant fold ng opt m zat on replaces tensors whose values can be
  // predeterm ned, w h constant nodes. To avo d  nsert ng too large constants,
  // t  s ze of each constant created can be l m ed.  f t  value  s zero, a
  // default l m  of 10 M B w ll be appl ed.  f constant fold ng opt m zat on
  //  s d sabled, t  value  s  gnored.
   nt64 max_folded_constant_ n_bytes = 6;

  //  f true, perform funct on  nl n ng on t  graph.
  bool do_funct on_ nl n ng = 4;

  // Opt m zat on level
  enum Level {
    // L1  s t  default level.
    // Opt m zat on perfor d at L1 :
    // 1. Common subexpress on el m nat on
    // 2. Constant fold ng
    L1 = 0;

    // No opt m zat ons
    L0 = -1;
  }

  // Overall opt m zat on level. T  actual opt m zat ons appl ed w ll be t 
  // log cal OR of t  flags that t  level  mpl es and any flags already set.
  Level opt_level = 3;

  // Control t  use of t  comp ler/j .  Exper  ntal.
  enum GlobalJ Level {
    DEFAULT = 0;  // Default sett ng ("off" now, but later expected to be "on")
    OFF = -1;
    // T  follow ng sett ngs turn on comp lat on, w h h g r values be ng
    // more aggress ve.  H g r values may reduce opportun  es for parallel sm
    // and may use more  mory.  (At present, t re  s no d st nct on, but t 
    //  s expected to change.)
    ON_1 = 1;
    ON_2 = 2;
  }
  GlobalJ Level global_j _level = 5;

  // CPU code w ll be autoclustered only  f global_j _level >= ON_1 and e  r:
  //  - t  flag  s true, or
  //  - TF_XLA_FLAGS conta ns --tf_xla_cpu_global_j =true.
  bool cpu_global_j  = 7;
}

 ssage GraphOpt ons {
  // Removed, use opt m zer_opt ons below.
  reserved "sk p_common_subexpress on_el m nat on";
  reserved 1;

  //  f true, use control flow to sc dule t  act vat on of Recv nodes.
  // (Currently  gnored.)
  bool enable_recv_sc dul ng = 2;

  // Opt ons controll ng how graph  s opt m zed.
  Opt m zerOpt ons opt m zer_opt ons = 3;

  // T  number of steps to run before return ng a cost model deta l ng
  // t   mory usage and performance of each node of t  graph. 0  ans
  // no cost model.
   nt64 bu ld_cost_model = 4;

  // T  number of steps to sk p before collect ng stat st cs for t 
  // cost model.
   nt64 bu ld_cost_model_after = 9;

  // Annotate each Node w h Op output shape data, to t  extent   can
  // be stat cally  nferred.
  bool  nfer_shapes = 5;

  // Only place t  subgraphs that are run, rat r than t  ent re graph.
  //
  // T   s useful for  nteract ve graph bu ld ng, w re one m ght
  // produce graphs that cannot be placed dur ng t  debugg ng
  // process.   n part cular,   allows t  cl ent to cont nue work  n
  // a sess on after add ng a node to a graph whose place nt
  // constra nts are unsat sf able.
  bool place_pruned_graph = 6;

  //  f true, transfer float values bet en processes as bfloat16.
  bool enable_bfloat16_sendrecv = 7;

  //  f > 0, record a t  l ne every t  many steps.
  // EXPER MENTAL: T  currently has no effect  n MasterSess on.
   nt32 t  l ne_step = 8;

  // Opt ons that control t  type and amount of graph rewr  ng.
  // Not currently conf gurable v a t  publ c Python AP  ( .e. t re  s no AP 
  // stab l y guarantee  f    mport Rewr erConf g expl c ly).
  Rewr erConf g rewr e_opt ons = 10;
}

 ssage ThreadPoolOpt onProto {
  // T  number of threads  n t  pool.
  //
  // 0  ans t  system p cks a value based on w re t  opt on proto  s used
  // (see t  declarat on of t  spec f c f eld for more  nfo).
   nt32 num_threads = 1;

  // T  global na  of t  threadpool.
  //
  //  f empty, t n t  threadpool  s made and used accord ng to t  scope  's
  //  n - e.g., for a sess on threadpool,    s used by that sess on only.
  //
  //  f non-empty, t n:
  // - a global threadpool assoc ated w h t  na   s looked
  //   up or created. T  allows, for example, shar ng one threadpool across
  //   many sess ons (e.g., l ke t  default behav or,  f
  //    nter_op_parallel sm_threads  s not conf gured), but st ll part  on ng
  //    nto a large and small pool.
  // -  f t  threadpool for t  global_na  already ex sts, t n    s an
  //   error  f t  ex st ng pool was created us ng a d fferent num_threads
  //   value as  s spec f ed on t  call.
  // - threadpools created t  way are never garbage collected.
  str ng global_na  = 2;
}

 ssage RPCOpt ons {
  //  f true, always use RPC to contact t  sess on target.
  //
  //  f false (t  default opt on), TensorFlow may use an opt m zed
  // transport for cl ent-master commun cat on that avo ds t  RPC
  // stack. T  opt on  s pr mar ly for used test ng t  RPC stack.
  bool use_rpc_for_ nprocess_master = 1;

  // T  compress on algor hm to be used. One of "deflate", "gz p".
  str ng compress on_algor hm = 2;

  //  f compress on_algor hm  s set, t  compress on level to be used.
  // From 0 (no compress on), up to 3.
   nt32 compress on_level = 3;

  // Sett ng cac _rpc_response to true w ll enable sender s de cach ng of
  // response for RecvTensorAsync and RecvBufAsync to allow rece ver to retry
  // requests . T   s only necessary w n t  network fabr c  s exper enc ng a
  // s gn f cant error rate.  W hout    'll fa l a step on an network error,
  // wh le w h    'll be able to complete long steps (l ke complex
  //  n  al zat ons)  n t  face of so  network errors dur ng RecvTensor.
  bool cac _rpc_response = 4;

  // D sables TCP connect on shar ng w n open ng a new RPC channel.
  bool d sable_sess on_connect on_shar ng = 5;

  // Sett ng num_channels_per_target > 0 allows uses of mult ple channels to
  // commun cate to t  sa  target. T  can be used to  mprove t  aggregate
  // throughput on h gh speed l nks (e.g 100G) w re s ngle connect on  s not
  // suff c ent to max m ze l nk ut l zat on. Note that a s ngle RPC only goes
  // on a s ngle channel, t  only  lps  n s uat ons w re t re are mult ple
  // transfers to t  sa  target overlapp ng  n t  .
   nt32 num_channels_per_target = 6;
}

//  tadata about t  sess on.
//
// T  can be used by t  runt   and t  Ops for debugg ng, mon or ng, etc.
//
// T  (na , vers on) tuple  s expected to be a un que  dent f er for
// sess ons w h n t  sa  process.
//
// NOTE: T   s currently used and propagated only by t  d rect sess on.
 ssage Sess on tadata {
  str ng na  = 1;

  // T  vers on  s opt onal.  f set, needs to be >= 0.
   nt64 vers on = 2;
}

// Sess on conf gurat on para ters.
// T  system p cks appropr ate values for f elds that are not set.
 ssage Conf gProto {
  // Map from dev ce type na  (e.g., "CPU" or "GPU" ) to max mum
  // number of dev ces of that type to use.   f a part cular dev ce
  // type  s not found  n t  map, t  system p cks an appropr ate
  // number.
  map<str ng,  nt32> dev ce_count = 1;

  // T  execut on of an  nd v dual op (for so  op types) can be
  // parallel zed on a pool of  ntra_op_parallel sm_threads.
  // 0  ans t  system p cks an appropr ate number.
  //
  //  f   create an ord nary sess on, e.g., from Python or C++,
  // t n t re  s exactly one  ntra op thread pool per process.
  // T  f rst sess on created determ nes t  number of threads  n t  pool.
  // All subsequent sess ons reuse/share t  one global pool.
  //
  // T re are notable except ons to t  default behav or descr bed above:
  // 1. T re  s an env ron nt var able  for overr d ng t  thread pool,
  //    na d TF_OVERR DE_GLOBAL_THREADPOOL.
  // 2. W n connect ng to a server, such as a remote `tf.tra n.Server`
  //     nstance, t n t  opt on w ll be  gnored altoget r.
   nt32  ntra_op_parallel sm_threads = 2;

  // Nodes that perform block ng operat ons are enqueued on a pool of
  //  nter_op_parallel sm_threads ava lable  n each process.
  //
  // 0  ans t  system p cks an appropr ate number.
  // Negat ve  ans all operat ons are perfor d  n caller's thread.
  //
  // Note that t  f rst Sess on created  n t  process sets t 
  // number of threads for all future sess ons unless use_per_sess on_threads  s
  // true or sess on_ nter_op_thread_pool  s conf gured.
   nt32  nter_op_parallel sm_threads = 5;

  //  f true, use a new set of threads for t  sess on rat r than t  global
  // pool of threads. Only supported by d rect sess ons.
  //
  //  f false, use t  global threads created by t  f rst sess on, or t 
  // per-sess on thread pools conf gured by sess on_ nter_op_thread_pool.
  //
  // T  opt on  s deprecated. T  sa  effect can be ach eved by sett ng
  // sess on_ nter_op_thread_pool to have one ele nt, whose num_threads equals
  //  nter_op_parallel sm_threads.
  bool use_per_sess on_threads = 9;

  // T  opt on  s exper  ntal -   may be replaced w h a d fferent  chan sm
  //  n t  future.
  //
  // Conf gures sess on thread pools.  f t   s conf gured, t n RunOpt ons for
  // a Run call can select t  thread pool to use.
  //
  // T   ntended use  s for w n so  sess on  nvocat ons need to run  n a
  // background pool l m ed to a small number of threads:
  // - For example, a sess on may be conf gured to have one large pool (for
  // regular compute) and one small pool (for per od c, low pr or y work);
  // us ng t  small pool  s currently t   chan sm for l m  ng t   nter-op
  // parallel sm of t  low pr or y work.  Note that   does not l m  t 
  // parallel sm of work spawned by a s ngle op kernel  mple ntat on.
  // - Us ng t  sett ng  s normally not needed  n tra n ng, but may  lp so 
  // serv ng use cases.
  // -    s also generally recom nded to set t  global_na  f eld of t 
  // proto, to avo d creat ng mult ple large pools.    s typ cally better to
  // run t  non-low-pr or y work, even across sess ons,  n a s ngle large
  // pool.
  repeated ThreadPoolOpt onProto sess on_ nter_op_thread_pool = 12;

  // Ass gn nt of Nodes to Dev ces  s recomputed every place nt_per od
  // steps unt l t  system warms up (at wh ch po nt t  recomputat on
  // typ cally slows down automat cally).
   nt32 place nt_per od = 3;

  // W n any f lters are present sess ons w ll  gnore all dev ces wh ch do not
  // match t  f lters. Each f lter can be part ally spec f ed, e.g. "/job:ps"
  // "/job:worker/repl ca:3", etc.
  repeated str ng dev ce_f lters = 4;

  // Opt ons that apply to all GPUs.
  GPUOpt ons gpu_opt ons = 6;

  // W t r soft place nt  s allo d.  f allow_soft_place nt  s true,
  // an op w ll be placed on CPU  f
  //   1. t re's no GPU  mple ntat on for t  OP
  // or
  //   2. no GPU dev ces are known or reg stered
  // or
  //   3. need to co-locate w h reftype  nput(s) wh ch are from CPU.
  bool allow_soft_place nt = 7;

  // W t r dev ce place nts should be logged.
  bool log_dev ce_place nt = 8;

  // Opt ons that apply to all graphs.
  GraphOpt ons graph_opt ons = 10;

  // Global t  out for all block ng operat ons  n t  sess on.   f non-zero,
  // and not overr dden on a per-operat on bas s, t  value w ll be used as t 
  // deadl ne for all block ng operat ons.
   nt64 operat on_t  out_ n_ms = 11;

  // Opt ons that apply w n t  sess on uses t  d str buted runt  .
  RPCOpt ons rpc_opt ons = 13;

  // Opt onal l st of all workers to use  n t  sess on.
  ClusterDef cluster_def = 14;

  //  f true, any res ces such as Var ables used  n t  sess on w ll not be
  // shared w h ot r sess ons. Ho ver, w n clusterspec propagat on  s
  // enabled, t  f eld  s  gnored and sess ons are always  solated.
  bool  solate_sess on_state = 15;

  // W n true, WorkerSess ons are created w h dev ce attr butes from t 
  // full cluster.
  // T   s  lpful w n a worker wants to part  on a graph
  // (for example dur ng a Part  onedCallOp).
  bool share_cluster_dev ces_ n_sess on = 17;

  // Everyth ng  ns de Exper  ntal  s subject to change and  s not subject
  // to AP  stab l y guarantees  n
  // https://www.tensorflow.org/gu de/vers on_compat.
   ssage Exper  ntal {
    // Task na  for group resolut on.
    str ng collect ve_group_leader = 1;

    //   removed t  flag cl ent_handles_error_formatt ng. Mark ng t  tag
    // number as reserved.
    // TODO(sh kharagarwal): Should   just remove t  tag so that   can be
    // used  n future for ot r purpose?
    reserved 2;

    // Wh ch executor to use, t  default executor w ll be used
    //  f    s an empty str ng or "DEFAULT"
    str ng executor_type = 3;

    // Gu dance to formatt ng of large RecvBuf f elds for transfer.
    // Any pos  ve value sets t  max chunk s ze.  0 defaults to 4096.
    // Any negat ve value  nd cates no max,  .e. one chunk only.
     nt32 recv_buf_max_chunk = 4;

    //  f true, and supported by t  platform, t  runt   w ll attempt to
    // use NUMA aff n y w re appl cable.  One consequence w ll be t 
    // ex stence of as many CPU dev ces as t re are ava lable NUMA nodes.
    bool use_numa_aff n y = 5;

    //  f true, make collect ve op execut on order sequent al and determ n st c
    // for potent ally concurrent collect ve  nstances.
    bool collect ve_determ n st c_sequent al_execut on = 6;

    //  f true, use NCCL for Collect veOps.  T  feature  s h ghly
    // exper  ntal.
    bool collect ve_nccl = 7;

    //  n t  follow ng, sess on state  ans t  value of a var able, ele nts
    //  n a hash table, or any ot r res ce, access ble by worker sess ons
    //  ld by a TF server.
    //
    // W n ClusterSpec propagat on  s enabled, t  value of
    //  solate_sess on_state  s  gnored w n dec d ng w t r to share sess on
    // states  n a TF server (for backwards compat b l y reasons).
    // -  f share_sess on_state_ n_clusterspec_propagat on  s true, t  sess on
    // states are shared.
    // -  f share_sess on_state_ n_clusterspec_propagat on  s false, sess on
    // states are  solated.
    //
    // W n clusterspec propagat on  s not used, t  value of
    // share_sess on_state_ n_clusterspec_propagat on  s  gnored w n dec d ng
    // w t r to share sess on states  n a TF server.
    // -  f  solate_sess on_state  s true, sess on states are  solated.
    // -  f  solate_sess on_state  s false, sess on states are shared.
    //
    // TODO(b/129330037): Add a s ngle AP  that cons stently treats
    //  solate_sess on_state and ClusterSpec propagat on.
    bool share_sess on_state_ n_clusterspec_propagat on = 8;

    //  f us ng a d rect sess on, d sable sp nn ng wh le wa  ng for work  n
    // t  thread pool. T  may result  n h g r latency for complet ng ops,
    // but  n t  case w re t re  s a lot of sp nn ng may result  n lo r
    // CPU usage.
    bool d sable_thread_sp nn ng = 9;

    // T  was promoted to a non-exper  ntal AP . Please use
    // Conf gProto.share_cluster_dev ces_ n_sess on  nstead.
    bool share_cluster_dev ces_ n_sess on = 10;

    //  tadata about t  sess on.
    //
    //  f set, t  can be used by t  runt   and t  Ops for debugg ng,
    // mon or ng, etc.
    //
    // NOTE: T   s currently used and propagated only by t  d rect sess on.
    Sess on tadata sess on_ tadata = 11;

    //  f true, t  sess on may treat t  graph as be ng stat c for opt m zat on
    // purposes.
    //
    //  f t  opt on  s set to true w n a sess on  s created, t  full
    // GraphDef must be passed  n a s ngle call to Sess on::Create(), and
    // Sess on::Extend() may not be supported.
    bool opt m ze_for_stat c_graph = 12;

    // T  f eld w ll eventually be deprecated and replaced by
    // ml r_br dge_rollout (b/166038521).
    //
    // W t r to enable t  ML R-based TF->XLA br dge.
    //
    // T   s a replace nt to t  ex st ng br dge, and not ready for
    // product on usage yet.
    //  f t  opt on  s set to true w n a sess on  s created, ML R  s used to
    // perform t  set of graph transformat ons to put t  graph  n a form that
    // can be executed w h delegat on of so  computat ons to an accelerator.
    // T  bu lds on t  model of XLA w re a subset of t  graph  s
    // encapsulated and attac d to a "comp le" operat on, whose result  s fed
    // to an "execute" operat on. T  kernel for t se operat ons  s respons ble
    // to lo r t  encapsulated graph to a part cular dev ce.
    bool enable_ml r_br dge = 13;

    // An enum that descr bes t  state of t  ML R br dge rollout.
    enum Ml rBr dgeRollout {
      //  f t  f eld  s left unspec f ed, t  ML R br dge may be select vely
      // enabled on a per graph bas s.
      ML R_BR DGE_ROLLOUT_UNSPEC F ED = 0;
      // Enabl ng t  ML R br dge enables   for all graphs  n t  sess on.
      ML R_BR DGE_ROLLOUT_ENABLED = 1;
      // D sabl ng t  ML R br dge d sables   for all graphs  n t  sess on.
      ML R_BR DGE_ROLLOUT_D SABLED = 2;
      // Enable t  ML R br dge on a per graph bas s based on an analys s of
      // t  features used  n t  graph.  f t  features used by t  graph are
      // supported by t  ML R br dge, t  ML R br dge w ll be used to run t 
      // graph.
      ML R_BR DGE_ROLLOUT_SAFE_MODE_ENABLED = 3;
      // Enable t  ML R br dge  n a fallback mode on a per graph bas s based
      // on an analys s of t  features used  n t  graph.
      // Runn ng t  ML R br dge  n t  fallback mode  ans that    s
      // executed and   comm s all t  changes to t  TF graph  n case
      // of success. And   does not  n case of fa lures and let t  old br dge
      // to process t  TF graph.
      ML R_BR DGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED = 4;
    }
    // T  f eld  s underdevelop nt, for now use enable_ml r_br dge
    // (b/166038521).
    //
    // W t r to enable t  ML R-based TF->XLA br dge.
    Ml rBr dgeRollout ml r_br dge_rollout = 17;

    // W t r to enable t  ML R-based Graph opt m zat ons.
    //
    // T  w ll beco  a part of standard Tensorflow graph opt m zat on
    // p pel ne, currently t   s only used for gradual m grat on and test ng
    // new passes that are replac ng ex st ng opt m zat ons  n Grappler.
    bool enable_ml r_graph_opt m zat on = 16;

    //  f true, t  sess on w ll not store an add  onal copy of t  graph for
    // each subgraph.
    //
    //  f t  opt on  s set to true w n a sess on  s created, t 
    // `RunOpt ons.output_part  on_graphs` opt ons must not be set.
    bool d sable_output_part  on_graphs = 14;

    // M n mum number of batc s run through t  XLA graph before XLA fus on
    // autotuner  s enabled. Default value of zero d sables t  autotuner.
    //
    // T  XLA fus on autotuner can  mprove performance by execut ng a  ur st c
    // search on t  comp ler para ters.
     nt64 xla_fus on_autotuner_thresh = 15;

    // W t r runt   execut on uses TFRT.
    bool use_tfrt = 18;

    // T  f eld "coord nat on_serv ce was prev ously spec f ed as a str ng;
    // t  has been replaced w h a  ssage below.
    reserved 19;

    //   removed t  flag fetch_remote_dev ces_ n_mult _cl ent. Mark ng t  tag
    // number as reserved.
    reserved 20;

    // W t r funct onal control flow op lo r ng should be d sabled. T   s
    // useful w n execut ng w h n a portable runt   w re control flow op
    // kernels may not be loaded due to select ve reg strat on.
    bool d sable_funct onal_ops_lo r ng = 21;

    // Prov des a h nt to XLA auto cluster ng to prefer form ng a s ngle large
    // cluster that encompases most of t  graph.
    bool xla_prefer_s ngle_graph_cluster = 22;

    // D str buted coord nat on serv ce conf gurat ons.
    Coord nat onServ ceConf g coord nat on_conf g = 23;

    // Next: 24
  }

  Exper  ntal exper  ntal = 16;

  // Next: 18
}

// Opt ons for a s ngle Run() call.
 ssage RunOpt ons {
  // TODO(pbar) Turn t   nto a TraceOpt ons proto wh ch allows
  // trac ng to be controlled  n a more orthogonal manner?
  enum TraceLevel {
    NO_TRACE = 0;
    SOFTWARE_TRACE = 1;
    HARDWARE_TRACE = 2;
    FULL_TRACE = 3;
  }
  TraceLevel trace_level = 1;

  // T   to wa  for operat on to complete  n m ll seconds.
   nt64 t  out_ n_ms = 2;

  // T  thread pool to use,  f sess on_ nter_op_thread_pool  s conf gured.
  // To use t  caller thread set t  to -1 - t  uses t  caller thread
  // to execute Sess on::Run() and thus avo ds a context sw ch. Us ng t 
  // caller thread to execute Sess on::Run() should be done ONLY for s mple
  // graphs, w re t  over ad of an add  onal context sw ch  s
  // comparable w h t  over ad of Sess on::Run().
   nt32  nter_op_thread_pool = 3;

  // W t r t  part  on graph(s) executed by t  executor(s) should be
  // outputted v a Run tadata.
  bool output_part  on_graphs = 5;

  // EXPER MENTAL.  Opt ons used to  n  al ze DebuggerState,  f enabled.
  DebugOpt ons debug_opt ons = 6;

  // W n enabled, causes tensor allocat on  nformat on to be  ncluded  n
  // t  error  ssage w n t  Run() call fa ls because t  allocator ran
  // out of  mory (OOM).
  //
  // Enabl ng t  opt on can slow down t  Run() call.
  bool report_tensor_allocat ons_upon_oom = 7;

  // Everyth ng  ns de Exper  ntal  s subject to change and  s not subject
  // to AP  stab l y guarantees  n
  // https://www.tensorflow.org/gu de/vers on_compat.
   ssage Exper  ntal {
    //  f non-zero, declares that t  graph  s go ng to use collect ve
    // ops and must synchron ze step_ ds w h any ot r graph w h t 
    // sa  group_key value ( n a d str buted computat on w re tasks
    // run d sjo nt graphs).
     nt64 collect ve_graph_key = 1;
    //  f true, t n operat ons (us ng t   nter-op pool) across all
    // sess on::run() calls w ll be centrally sc duled, opt m z ng for ( d an
    // and ta l) latency.
    // Cons der us ng t  opt on for CPU-bound workloads l ke  nference.
    bool use_run_handler_pool = 2;
    // Opt ons for run handler thread pool.
     ssage RunHandlerPoolOpt ons {
      // Pr or y of t  request. T  run handler thread pool w ll sc dule ops
      // based on t  pr or y number. T  larger number  ans h g r pr or y.
       nt64 pr or y = 1;
    }
    RunHandlerPoolOpt ons run_handler_pool_opt ons = 3;
  }

  Exper  ntal exper  ntal = 8;

  reserved 4;
}

//  tadata output ( .e., non-Tensor) for a s ngle Run() call.
 ssage Run tadata {
  // Stat st cs traced for t  step. Populated  f trac ng  s turned on v a t 
  // "RunOpt ons" proto.
  // EXPER MENTAL: T  format and set of events may change  n future vers ons.
  StepStats step_stats = 1;

  // T  cost graph for t  computat on def ned by t  run call.
  CostGraphDef cost_graph = 2;

  // Graphs of t  part  ons executed by executors.
  repeated GraphDef part  on_graphs = 3;

   ssage Funct onGraphs {
    // TODO(nareshmod ):  nclude so  sort of funct on/cac -key  dent f er?
    repeated GraphDef part  on_graphs = 1;

    GraphDef pre_opt m zat on_graph = 2;
    GraphDef post_opt m zat on_graph = 3;
  }
  // T   s only populated for graphs that are run as funct ons  n TensorFlow
  // V2. T re w ll be an entry below for each funct on that  s traced.
  // T  ma n use cases of t  post_opt m zat on_graph and t  part  on_graphs
  //  s to g ve t  caller  ns ght  nto t  graphs that  re actually run by t 
  // runt  . Add  onal  nformat on (such as those  n step_stats) w ll match
  // t se graphs.
  //   also  nclude t  pre_opt m zat on_graph s nce    s usually eas er to
  // read, and  s  lpful  n s uat ons w re t  caller wants to get a h gh
  // level  dea of what t  bu lt graph looks l ke (s nce t  var ous graph
  // opt m zat on passes m ght change t  structure of t  graph s gn f cantly).
  repeated Funct onGraphs funct on_graphs = 4;
}

// Def nes a connect on bet en two tensors  n a `GraphDef`.
 ssage TensorConnect on {
  // A tensor na . T  value of t  tensor w ll be subst uted for
  // t  tensor na d  n `to_tensor`.
  str ng from_tensor = 1;

  // A tensor na . T  value of t  tensor w ll be bound to t 
  // value of t  tensor na d  n `from_tensor`.
  str ng to_tensor = 2;
}

// Def nes a subgraph  n anot r `GraphDef` as a set of feed po nts and nodes
// to be fetc d or executed.
//
// Compare w h t  argu nts to `Sess on::Run()`.
 ssage CallableOpt ons {
  // Tensors to be fed  n t  callable. Each feed  s t  na  of a tensor.
  repeated str ng feed = 1;

  // Fetc s. A l st of tensor na s. T  caller of t  callable expects a
  // tensor to be returned for each fetch[ ] (see RunStepResponse.tensor). T 
  // order of spec f ed fetc s does not change t  execut on order.
  repeated str ng fetch = 2;

  // Target Nodes. A l st of node na s. T  na d nodes w ll be run by t 
  // callable but t  r outputs w ll not be returned.
  repeated str ng target = 3;

  // Opt ons that w ll be appl ed to each run.
  RunOpt ons run_opt ons = 4;

  // Tensors to be connected  n t  callable. Each TensorConnect on denotes
  // a pa r of tensors  n t  graph, bet en wh ch an edge w ll be created
  //  n t  callable.
  repeated TensorConnect on tensor_connect on = 5;

  // T  Tensor objects fed  n t  callable and fetc d from t  callable
  // are expected to be backed by host (CPU)  mory by default.
  //
  // T  opt ons below allow chang ng that - feed ng tensors backed by
  // dev ce  mory, or return ng tensors that are backed by dev ce  mory.
  //
  // T  maps below map t  na  of a feed/fetch tensor (wh ch appears  n
  // 'feed' or 'fetch' f elds above), to t  fully qual f ed na  of t  dev ce
  // own ng t   mory back ng t  contents of t  tensor.
  //
  // For example, creat ng a callable w h t  follow ng opt ons:
  //
  // CallableOpt ons {
  //   feed: "a:0"
  //   feed: "b:0"
  //
  //   fetch: "x:0"
  //   fetch: "y:0"
  //
  //   feed_dev ces: {
  //     "a:0": "/job:localhost/repl ca:0/task:0/dev ce:GPU:0"
  //   }
  //
  //   fetch_dev ces: {
  //     "y:0": "/job:localhost/repl ca:0/task:0/dev ce:GPU:0"
  //  }
  // }
  //
  //  ans that t  Callable expects:
  // - T  f rst argu nt ("a:0")  s a Tensor backed by GPU  mory.
  // - T  second argu nt ("b:0")  s a Tensor backed by host  mory.
  // and of  s return values:
  // - T  f rst output ("x:0") w ll be backed by host  mory.
  // - T  second output ("y:0") w ll be backed by GPU  mory.
  //
  // FEEDS:
  //    s t  respons b l y of t  caller to ensure that t   mory of t  fed
  // tensors w ll be correctly  n  al zed and synchron zed before    s
  // accessed by operat ons executed dur ng t  call to Sess on::RunCallable().
  //
  // T   s typ cally ensured by us ng t  TensorFlow  mory allocators
  // (Dev ce::GetAllocator()) to create t  Tensor to be fed.
  //
  // Alternat vely, for CUDA-enabled GPU dev ces, t  typ cally  ans that t 
  // operat on that produced t  contents of t  tensor has completed,  .e., t 
  // CUDA stream has been synchron zed (e.g., v a cuCtxSynchron ze() or
  // cuStreamSynchron ze()).
  map<str ng, str ng> feed_dev ces = 6;
  map<str ng, str ng> fetch_dev ces = 7;

  // By default, RunCallable() w ll synchron ze t  GPU stream before return ng
  // fetc d tensors on a GPU dev ce, to ensure that t  values  n those tensors
  // have been produced. T  s mpl f es  nteract ng w h t  tensors, but
  // potent ally  ncurs a performance h .
  //
  //  f t  opt ons  s set to true, t  caller  s respons ble for ensur ng
  // that t  values  n t  fetc d tensors have been produced before t y are
  // used. T  caller can do t  by  nvok ng `Dev ce::Sync()` on t  underly ng
  // dev ce(s), or by feed ng t  tensors back to t  sa  Sess on us ng
  // `feed_dev ces` w h t  sa  correspond ng dev ce na .
  bool fetch_sk p_sync = 8;

  // Next: 9
}
